{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network for multi-label classification on the Celeb-A dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -O 'archive.zip' 'https://storage.googleapis.com/kaggle-data-sets/29561/37705/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20221102%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20221102T043448Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4a8303ca846eaae80b62ff3705ad6b86de91724f8d76b4373083c4c33225314b6c43af9bab79e32100af51aa02e230a1eaa81ae33aea94680fbb118fb74dd6bf4d2a9426809a43ce699da806f156781c98f5076845d8936c304657265ecaa323ba0a369aa7d436916a6a87ba8a5873865da1b30052c9f128f2005fa12e1526850a79fe96e74aa698bc4ce59b3822d159b22ece231b7246d7800b685c5e9974e7837dfb4d2d7c18147749443bdbe9ae44f5db3e045f25774df6d464c7f0762279fea7b201ce76ceb15d1301f4354112db105c108dd5564f3a165f9f2af1fb581f5fa6d5b471a194007290b03637a38105c09a3c6f2aebb26e3df2fccf49d349a6'\n",
    "!unzip -qq archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs website).\n",
    "# If running on Colab, may want to use GPU (select: Runtime > Change runtime type > Hardware accelerator > GPU)\n",
    "# Package versions we used: matplotlib==3.5.1, torch==1.11.0, torchvision==0.12.0, timm==0.5.4\n",
    "\n",
    "dependencies = [\"torch\", \"torchvision\", \"sklearn\", \"timm\", \"pillow\",\"scikit-learn\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "missing_dependencies = []\n",
    "for dependency in dependencies:\n",
    "    try:\n",
    "        __import__(dependency)\n",
    "    except ImportError:\n",
    "        missing_dependencies.append(dependency)\n",
    "\n",
    "if len(missing_dependencies) > 0:\n",
    "    print(\"Missing required dependencies:\")\n",
    "    print(*missing_dependencies, sep=\", \")\n",
    "    print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from timm.data.loader import create_loader\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from timm.utils import CheckpointSaver\n",
    "from types import SimpleNamespace\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from timm.optim import create_optimizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from timm.data import (\n",
    "    resolve_data_config,\n",
    ")\n",
    "from timm.models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"list_attr_celeba.csv\")\n",
    "# convert -1 -> 0\n",
    "dat[dat.columns[1:]] = ((dat[dat.columns[1:]]+1)/2).astype(np.int32)\n",
    "\n",
    "selected = ['image_id',\n",
    "'Eyeglasses',\n",
    " 'Wearing_Earrings',\n",
    " 'Wearing_Hat',\n",
    " 'Wearing_Necklace',\n",
    " 'Wearing_Necktie',\n",
    " 'No_Beard',\n",
    " 'Smiling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_label(row):\n",
    "    for s in selected[1:]:\n",
    "        if row[s]!=0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_loc(i):\n",
    "    return os.path.join(os.getcwd(),'img_align_celeba/img_align_celeba/')+i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_label = dat.apply(is_label,axis=1)\n",
    "dat_selected = dat[dat_label][selected]\n",
    "dat_selected['image_path'] = dat_selected['image_id'].map(lambda x:get_loc(x))\n",
    "selected[0] = 'image_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dat_selected[selected]\n",
    "\n",
    "set_lab = {}\n",
    "for i,row in df.iterrows():\n",
    "    q = str(row.tolist()[1:])\n",
    "    if q not in set_lab:\n",
    "        set_lab[(str(q))]=len(set_lab)\n",
    "\n",
    "def get_lab(row):\n",
    "    q = str(row.tolist()[1:])\n",
    "    return set_lab[q]\n",
    "\n",
    "df['unique_label'] = df.apply(get_lab,axis=1)\n",
    "cnt = Counter(df['unique_label'])\n",
    "\n",
    "#We drop unique counts < 10 to avoid errors in Stratified KFold\n",
    "def drop(val):\n",
    "    if cnt[val]>10:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "is_drop = df['unique_label'].apply(lambda x:drop(x))\n",
    "\n",
    "\n",
    "\n",
    "df = df[is_drop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelModel(nn.Module):\n",
    "    def __init__(self, model, n_classes, class_weights=None, verbose = False):\n",
    "        super().__init__()\n",
    "        self.base_model = model\n",
    "        self.num_classes = n_classes\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_loss(self, loss_fn, output, target):\n",
    "\n",
    "        return loss_fn(output, target)\n",
    "\n",
    "    def validate(self, loader):\n",
    "        self.eval();\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            m = nn.Sigmoid()\n",
    "            labels = []\n",
    "            preds = []\n",
    "            for batch_idx, (input, target) in enumerate(loader):\n",
    "                input = input.cuda()\n",
    "                labels.append(target.detach().cpu())\n",
    "                target = target.float().cuda()\n",
    "                output = m(self(input))\n",
    "                loss = self.get_loss(loss_fn, output, target)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pred_model = (output > 0.5).detach().cpu()\n",
    "                preds.append(pred_model)\n",
    "\n",
    "            num_of_batches_per_epoch = len(loader)\n",
    "            avg_loss = total_loss / num_of_batches_per_epoch\n",
    "            print(\"VALIDATION DATA STATS\")\n",
    "\n",
    "            print(\"AVERAGE LOSS:\", avg_loss)\n",
    "            preds = torch.cat(preds).int()\n",
    "            labels = torch.cat(labels).int()\n",
    "            acc_score = accuracy_score(labels, preds)\n",
    "            print(\"MULTILABEL accuracy score:\", acc_score)\n",
    "            per_class = []\n",
    "            for i in range(len(preds.T)):\n",
    "                per_class.append(accuracy_score(labels.T[i], preds.T[i]))\n",
    "            print(dataset_train.label_names)\n",
    "            print(per_class)\n",
    "            print('\\n\\n')\n",
    "        return avg_loss\n",
    "\n",
    "    def predict_proba(self, loader):\n",
    "        self.eval();\n",
    "        with torch.no_grad():\n",
    "            m = nn.Sigmoid()\n",
    "            preds = []\n",
    "            for batch_idx, (input, target) in enumerate(loader):\n",
    "                input = input.cuda()\n",
    "                output = m(self(input))\n",
    "                pred_model = output.detach().cpu()\n",
    "                preds.append(pred_model)\n",
    "            preds = torch.cat(preds)\n",
    "        return preds\n",
    "\n",
    "    def train_one_epoch(\n",
    "        self,\n",
    "        loader,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "    ):\n",
    "        sta = time.time()\n",
    "        second_order = hasattr(optimizer, \"is_second_order\") and optimizer.is_second_order\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        m = nn.Sigmoid()\n",
    "        labels = []\n",
    "        preds = []\n",
    "        ct = 0\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            input = input.cuda()\n",
    "            ct += 1\n",
    "            labels.append(target.detach().cpu())\n",
    "            target = target.float().cuda()\n",
    "            output = m(self(input))\n",
    "            loss = self.get_loss(loss_fn, output, target)\n",
    "            total_loss += loss.item()\n",
    "            pred_model = (output > 0.5).detach().cpu()\n",
    "            preds.append(pred_model)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(create_graph=second_order)\n",
    "            optimizer.step()\n",
    "            if ct % 80 == 0 and self.verbose:\n",
    "                print(\"LOSS:\", loss.item())\n",
    "        num_of_batches_per_epoch = len(loader)\n",
    "        avg_loss = total_loss / num_of_batches_per_epoch\n",
    "        print(\"TRAINING DATA STATS\")\n",
    "        print(\"AVERAGE LOSS:\", avg_loss)\n",
    "        preds = torch.cat(preds).int()\n",
    "        labels = torch.cat(labels).int()\n",
    "        acc_score = accuracy_score(labels, preds)\n",
    "        print(\"MULTILABEL accuracy score:\", acc_score)\n",
    "        per_class = []\n",
    "        for i in range(len(preds.T)):\n",
    "            per_class.append(accuracy_score(labels.T[i], preds.T[i]))\n",
    "        print(dataset_train.label_names)\n",
    "        print(per_class)\n",
    "        print('\\n\\n')\n",
    "        sto = time.time()\n",
    "        print(\"training time\", sto - sta)\n",
    "        return avg_loss\n",
    "    \n",
    "\n",
    "    def fit(self, loader_train, load_val, num_epochs=10):\n",
    "        if os.path.exists(\"weights_model\"):\n",
    "            print(\"removing weights directory\")\n",
    "            os.system('rm -rf weights_model')\n",
    "        os.mkdir(\"weights_model\")\n",
    "        args = SimpleNamespace()\n",
    "        args.weight_decay = 0\n",
    "        args.lr = 1e-4\n",
    "        args.opt = 'adam'\n",
    "        args.momentum = 0.9\n",
    "        args.sched = \"step\"\n",
    "\n",
    "        optimizer = create_optimizer(args, self)\n",
    "        saver = CheckpointSaver(\n",
    "            model=self,\n",
    "            optimizer=optimizer,\n",
    "            checkpoint_dir=\"weights_model\"\n",
    "        )\n",
    "        errs = []\n",
    "        num_of_data_train = len(loader_train.dataset.data)\n",
    "        for epoch in range(0, num_epochs):\n",
    "            loss_train = self.train_one_epoch(\n",
    "                loader_train,\n",
    "                optimizer,\n",
    "                loss_fn,\n",
    "            )\n",
    "            loss_val = self.validate(loader_val)\n",
    "            errs.append([loss_train, loss_val])\n",
    "            saver.save_checkpoint(epoch, metric=loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMultiLabel(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            annotation_path=None,\n",
    "            df = None,\n",
    "            transform=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.label_names = []\n",
    "        if annotation_path is None:\n",
    "            assert df is not None\n",
    "        else:\n",
    "            df = pd.read_csv(annotation_path)\n",
    "        \n",
    "        cols = df.columns\n",
    "        self.label_names = list(cols[1:-1])\n",
    "        for i,row in df.iterrows():\n",
    "            lb = []\n",
    "            for j in cols:\n",
    "                if j=='unique_label':\n",
    "                    continue\n",
    "                if j=='image_path':\n",
    "                    self.data.append(row[j])\n",
    "                else:\n",
    "                    lb.append(float(row[j]))\n",
    "            self.labels.append(lb)\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        return img, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetMultiLabel(df = df)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "model = create_model(\n",
    "    'efficientnet_b0',\n",
    "    num_classes=len(dataset.labels[0]),\n",
    ")\n",
    "data_config = resolve_data_config(\n",
    "       args = {}, model=model\n",
    "    )\n",
    "\n",
    "model = MultiLabelModel(\n",
    "        model,\n",
    "        n_classes=len(dataset.labels[0]),\n",
    "    ).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "def create_df(pred_probs,dataset):\n",
    "    \"\"\"\n",
    "    Creates a dataframe with image_loc and predicted probabilities\n",
    "    \"\"\"\n",
    "    ls = dataset_val.label_names\n",
    "    cl = defaultdict(list)\n",
    "    cl['image_loc'] = dataset.data\n",
    "    for i in range(0,len(ls)):\n",
    "        cl[ls[i]] = pred_val.T[i].tolist()\n",
    "    return pd.DataFrame.from_dict(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct = 1\n",
    "for train_index, test_index in skf.split(df,df['unique_label']):\n",
    "    if ct!=1:\n",
    "        model.apply(reset_weights);\n",
    "    dataset_train = DatasetMultiLabel(df = df.iloc[train_index])\n",
    "    dataset_val = DatasetMultiLabel(df = df.iloc[test_index])\n",
    "    loader_train = create_loader(\n",
    "        dataset_train,\n",
    "        input_size=data_config[\"input_size\"],\n",
    "        batch_size=64,\n",
    "        is_training=True,\n",
    "        mean=data_config[\"mean\"],\n",
    "        std=data_config[\"std\"],\n",
    "       interpolation=data_config[\"interpolation\"],\n",
    "    )\n",
    "    loader_val = create_loader(\n",
    "        dataset_val,\n",
    "        input_size=data_config[\"input_size\"],\n",
    "        batch_size=64,\n",
    "        is_training=False,\n",
    "        mean=data_config[\"mean\"],\n",
    "        std=data_config[\"std\"],\n",
    "        interpolation=data_config[\"interpolation\"],\n",
    "\n",
    "    )\n",
    "    model.fit(loader_train,loader_val,num_epochs=40)\n",
    "    checkpoint = torch.load(\"weights_model/model_best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    pred_val = model.predict_proba(loader_val)\n",
    "    df_pred = create_df(pred_val,dataset_val)\n",
    "    df_pred.to_csv(str(ct)+\"_fold.csv\",index=False)\n",
    "    ct+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for i in range(1,num_splits+1):\n",
    "    dfl.append(pd.read_csv(str(i)+\"_fold.csv\"))\n",
    "    \n",
    "\n",
    "cols = dfl[0].columns[1:]\n",
    "\n",
    "df_pred = pd.concat(dfl,axis=0)\n",
    "\n",
    "df_pred['image_loc'] = df_pred['image_loc'].map(lambda x:x.split('/')[-1])\n",
    "\n",
    "df_pred.set_index('image_loc').to_csv(\"pred_probs.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
